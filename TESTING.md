# User Testing Log

## Iteration 0 (6 March 2018)

We did a marginal job of implementing user testing when met with our first opportunity to do so at end-of-term demos (indeed, using the word 'marginal' places a _loose_ upper bound on the quality of this testing). 

Nonetheless, below are the results of the user testing that we did accomplish in this first iteration. 

**Questions**

* What are the rules of this game?
* What game is this / is this a game that you came up with yourselves?
* What are the other games that you can play? 
* Is the game deterministic? 
* What happens if two identical bots play against one another?
* What is the nature of the visualization? 
* How do you ensure that other users are not able to manipulate your system / my bot code when I am playing a match against them?

**Comments**

* Implement the ability to comment on specific matches
* Implement 'step' functionality to allow for more effective local bot development and debugging 

**Insights & Takeaways**

While we cannot change the past, we can certainly manage how we will conduct ourselves in the future. Our primary takeaway from this first iteration of user testing is to ensure that we take full advantage of future user testing opportunities. Now that we have a functioning MVP, we foresee that many more such opportunities will be available in the coming term. 

Another, more product-focused takeaway from this user testing session was the primacy of replay visualization. While certain individuals that visited our station were interested in the architecture that underlies the product, the majority of our visitors were enamored with watching game replays. This is hardly a surprising development, and it will certainly weight match replay visualization more heavily as we prioritize the tasks that we want to accomplish this coming term. 